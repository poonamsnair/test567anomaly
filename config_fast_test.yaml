# Fast Test Configuration for AI Agent Trajectory Anomaly Detection System

# Data Generation Configuration
data_generation:
  num_normal_trajectories: 5000  # Increased for robust training
  trajectory_patterns:
    simple_linear: {min_nodes: 3, max_nodes: 8, weight: 0.4}
    branched_analysis: {min_nodes: 5, max_nodes: 12, weight: 0.35}
    multi_agent_handoffs: {min_nodes: 6, max_nodes: 15, weight: 0.25}
  
  # Agent types and their capabilities
  agent_types:
    - Planner
    - Executor
    - Validator
    - Coordinator
  
  # Tool types available to agents
  tool_types:
    - web_search
    - read_document
    - analyze_data
    - write_code
    - validate_result

# Anomaly Injection Configuration
anomaly_injection:
  total_anomalous_trajectories: 350  # Increased for robust evaluation
  anomaly_types:
    infinite_loops: {severity: critical, ratio: 0.25}
    suboptimal_paths: {severity: medium, ratio: 0.3}
    tool_failure_cascades: {severity: high, ratio: 0.25}
    planning_paralysis: {severity: medium, ratio: 0.2}
  
  # Severity level definitions
  severity_levels:
    low: {degradation_range: [0.1, 0.2], user_impact: minimal}
    medium: {degradation_range: [0.2, 0.4], user_impact: noticeable}
    high: {degradation_range: [0.4, 0.7], user_impact: significant}
    critical: {degradation_range: [0.7, 1.0], user_impact: system_failure}

# Dataset size thresholds for adaptive hyperparameter selection
dataset_size_thresholds:
  small_medium_boundary: 100
  medium_large_boundary: 300

# Hyperparameter grids for different dataset sizes
hyperparameter_grids:
  small_dataset:
    max_combinations: 3
    max_combinations_gnn: 2
    max_combinations_ocsvm: 3
    isolation_forest:
      n_estimators: [100, 200]
      contamination: [0.05, 0.1]  # Expected anomaly ratio for unsupervised setting
      max_features: [0.5, 0.75]
      max_samples: [0.5, 0.75]
      bootstrap: [false]
      random_state: [42]
    one_class_svm:
      kernel: [rbf, linear]
      gamma: [scale, auto]
      nu: [0.1, 0.15]  # Expected anomaly ratio
      degree: [2]
    gnn_autoencoder:
      hidden_dims: [[16, 32], [32, 64]]
      learning_rate: [0.001, 0.01]
      dropout: [0.1, 0.2]
      batch_size: [8, 16]
      epochs: [30, 50]
      gnn_type: [GCN]  # Consistent with models section
  
  medium_dataset:
    max_combinations: 5
    max_combinations_gnn: 3
    max_combinations_ocsvm: 5
    isolation_forest:
      n_estimators: [200, 300]
      contamination: [0.03, 0.05, 0.08]  # Expected anomaly ratio
      max_features: [0.5, 0.75, 1.0]
      max_samples: [0.5, 0.75, 1.0]
      bootstrap: [true, false]
      random_state: [42]
    one_class_svm:
      kernel: [rbf, linear, poly]
      gamma: [scale, auto, 0.1]
      nu: [0.05, 0.1, 0.15]  # Expected anomaly ratio
      degree: [2, 3]
    gnn_autoencoder:
      hidden_dims: [[32, 64], [64, 128]]
      learning_rate: [0.001, 0.01]
      dropout: [0.1, 0.2, 0.3]
      batch_size: [16, 32]
      epochs: [50, 100]
      gnn_type: [GCN, GAT]  # Consistent with models section
  
  large_dataset:
    max_combinations: 10
    max_combinations_gnn: 5
    max_combinations_ocsvm: 10
    isolation_forest:
      n_estimators: [300, 500]
      contamination: [0.03, 0.05, 0.08, 0.1]  # Expected anomaly ratio
      max_features: [0.5, 0.75, 1.0]
      max_samples: [0.5, 0.75, 1.0]
      bootstrap: [true, false]
      random_state: [42]
    one_class_svm:
      kernel: [rbf, linear, poly]
      gamma: [scale, auto, 0.1, 0.01]
      nu: [0.05, 0.1, 0.15, 0.2]  # Expected anomaly ratio
      degree: [2, 3]
    gnn_autoencoder:
      hidden_dims: [[64, 128], [128, 256]]
      learning_rate: [0.001, 0.01]
      dropout: [0.1, 0.2, 0.3]
      batch_size: [32, 64]
      epochs: [100, 200]
      gnn_type: [GCN, GAT, GraphConv]  # Consistent with models section

# Graph Processing Configuration
graph_processing:
  checkpoint_dir: checkpoints  # Add checkpoint directory for embeddings
  node2vec:
    dimensions: [8]  # Lowered for speed
    walk_length: [5]  # Shorter walks
    num_walks: [5]  # Fewer walks
    p: [1.0]
    q: [1.0]
    window: [5]
    min_count: [1]
    workers: 2
  deepwalk:
    dimensions: [8]
    walk_length: [5]
    num_walks: [5]
    window: [5]
    min_count: [0]
    sg: 1
  graphsage:
    hidden_dims: [[8, 8]]
    output_dim: [8]
    learning_rate: [0.001]
    epochs: [5]
    batch_size: [8]
    dropout: [0.1]
    aggregator: ['mean']
  aggregation_methods:
    - mean
  centrality_measures:
    - betweenness
    - closeness

# Feature Engineering Configuration
feature_engineering:
  # Model-specific feature sets
  isolation_forest_features:
    structural_features:
      - num_nodes
      - num_edges
      - density
      - diameter
      - betweenness_centrality
    temporal_features:
      - total_duration
      - average_node_duration
    semantic_features:
      - error_frequency
      - handoff_frequency
    dag_features:
      - dag_depth

  one_class_svm_features:
    structural_features:
      - num_nodes
      - num_edges
      - density
      - diameter
      - betweenness_centrality
    temporal_features:
      - total_duration
      - average_node_duration
    semantic_features:
      - error_frequency
      - handoff_frequency
    dag_features:
      - dag_depth

  gnn_features:
    node_features:
      - agent_type
      - node_type
      - success
      - duration
      - error_count
    edge_features:
      - latency
      - edge_type
    graph_features:
      - num_nodes
      - num_edges
      - error_frequency

# Model Configuration
models:
  # Isolation Forest (improved)
  isolation_forest:
    n_estimators: [200, 300]  # Increased for better stability
    contamination: [0.03, 0.05, 0.08, 0.1]  # Expected anomaly ratio for unsupervised setting
    max_features: [0.5, 0.75, 1.0]  # More feature sampling options
    max_samples: [0.5, 0.75, 1.0]  # More sample sampling options
    bootstrap: [true, false]  # Try both bootstrap options
    random_state: [42]
  
  # One-Class SVM (improved)
  one_class_svm:
    kernel: [rbf, linear, poly]  # Added poly kernel
    gamma: [scale, auto, 0.1, 0.01]  # More gamma options
    nu: [0.05, 0.1, 0.15, 0.2]  # Expected anomaly ratio
    degree: [2, 3]  # For poly kernel
  
  # GNN Autoencoder (simplified for fast testing)
  gnn_autoencoder:
    hidden_dims: [[32, 64]]  # Single option for fast testing
    learning_rate: [0.001]  # Single value
    dropout: [0.1]  # Single value
    batch_size: [16]  # Single value
    epochs: [50]  # Reduced epochs for fast testing
    gnn_type: [GCN]  # Single type for consistency

# Evaluation Configuration
evaluation:
  # Data splitting
  data_split:
    train_ratio: 0.6  # Normal trajectories only
    validation_ratio: 0.2  # Normal + known anomalies
    test_ratio: 0.2  # Normal + unknown anomalies
  
  # Threshold calibration methods (improved)
  threshold_calibration:
    - roc_optimization
    - pr_optimization
    - f1_maximization
    - fixed_percentile_95
    - knee_point_detection
  
  # Evaluation metrics (improved set)
  metrics:
    - precision
    - recall
    - f1
    - accuracy
    - auc_roc
    - auc_pr
    - silhouette_score
    - severity_weighted_performance

# Visualization Configuration
visualization:
  # Graph layouts
  graph_layouts:
    - spring
    - hierarchical
  
  # Color schemes
  color_schemes:
    node_types: viridis
    agent_types: Set3
    anomaly_severity: Reds
  
  # Figure settings
  figure_settings:
    dpi: 150  # Lower DPI for faster generation
    format: png
    bbox_inches: tight
  
  # Interactive plots
  interactive:
    enabled: false
    renderer: browser

# System Configuration
system:
  random_seed: 42
  n_jobs: 2  # Reduced for faster processing
  memory_limit_gb: 4
  chunk_size: 500
  
  # Output directories
  output_dirs:
    data: results_fast_test/data/
    models: results_fast_test/models/
    charts: results_fast_test/charts/
    logs: results_fast_test/logs/
  
  # Logging configuration
  logging:
    level: INFO
    format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    file: logs/fast_test.log 